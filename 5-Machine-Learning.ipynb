{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7: Machine Learning\n",
    "\n",
    "This week we discussed the fundamentals of machine learning and reviewed some common ML algorithms. This notebook will give you more hands-on experience with programming these alogrithms for use in your own research. The goals of this notebook are:\n",
    "\n",
    "+ Explore convex and nonconvex optimization problems.\n",
    "+ Use the *scikit-learn* library to perform common ML analyses.\n",
    "+ Learn how to visualize high dimensional data.\n",
    "+ Utilize low dimensional representations of data for analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score, mean_squared_error,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, svm, metrics\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# set some style options\n",
    "mpl.rcParams['image.origin'] = 'lower'\n",
    "mpl.rcParams['image.aspect'] = 'auto'\n",
    "mpl.rcParams['image.cmap'] = 'jet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "Let's look at a very simple optimization problem where we are trying to estimate a model of the form:\n",
    "\n",
    "$y = bx+e$\n",
    "\n",
    "where,\n",
    "\n",
    "+ $y$: dependent variable\n",
    "+ $x$: independent variable\n",
    "+ $b$: model parameter to be estimated\n",
    "+ $e$: measurement error\n",
    "\n",
    "In class we saw that a common approach to obtaining a parameter estimate $\\hat{b}$ is to minimize the mean squared error:\n",
    "\n",
    "$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i-\\hat{y}_i)^2$\n",
    "\n",
    "with $\\hat{y_i} = \\hat{b}x_i$.\n",
    "\n",
    "For this exercise, we will be minimizing this error function via gradient descent. Since this is a univariate function, all we need to do is calculate the derivative of MSE with respect to $\\hat{b}$, which is given by:\n",
    "\n",
    "$\\frac{\\partial MSE}{\\partial \\hat{b}} = -\\frac{2}{n} \\sum_{i=1}^{n}(y_i - \\hat{y}_i)(x_i)$\n",
    "\n",
    "\n",
    "\n",
    "First, let's simulate some data. The linear regression model will be given by $y = 2x + e$, where $e$ is unbiased normally distributed measurement error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(0,20,100)\n",
    "b = 2\n",
    "e = np.random.normal(0,10,len(x))\n",
    "y = b*x+e\n",
    "\n",
    "plt.figure(figsize = (10,5))\n",
    "plt.plot(x,y,'o')\n",
    "plt.plot(x,b*x,color = 'black',label = \"True Regression Line\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Regression Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a strictly convex optimization problem, which means there is only one minima and it is the global one. We can see this by plotting the MSE for several possible values of $\\hat{b}$ (the error landscape). Our job is to find the $\\hat{b}$ that results in the minimum MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(x,y,b_hat):\n",
    "    n = len(x)\n",
    "    y_hat = b_hat*x\n",
    "    return((1/n)*np.sum((y-y_hat)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_hats = np.arange(-10,10,.1) #candidate values of b_hat\n",
    "errors = [] #array to store MSE values for each b_hat\n",
    "\n",
    "for b_hat in b_hats:\n",
    "    errors.append(MSE(x,y,b_hat))\n",
    "\n",
    "plt.plot(b_hats,errors) #plot MSE for each b_hat\n",
    "plt.plot(b,MSE(x,y,b),'o',markersize = 10,color = \"black\") #True value of b\n",
    "plt.xlabel(r\"$\\hat{b}$\")\n",
    "plt.ylabel(r\"$MSE(\\hat{b})$\")\n",
    "plt.title(\"Error Landscape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "##### Convex Optimization\n",
    "We will perform gradient descent to obtain an estimate $\\hat{b}$. You will need to specify a precision value, while the learning rate $\\eta$ will be given. If the difference between successive $\\hat{b}$'s is less than the precision value, the gradient descent alogrithm has converged to a final estimate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_gradient(x,y,b_hat):\n",
    "    \"\"\"Calculates the gradient of MSE for candidate b_hat\"\"\"\n",
    "    n = len(x)\n",
    "    y_hat = b_hat*x\n",
    "    return((-2/n)*np.sum((y-y_hat)*x))\n",
    "\n",
    "def MSE_GD(precision_value,eta = 0.00001):\n",
    "    \"\"\"Perform gradient descent of the MSE\"\"\"\n",
    "    b_hat = np.random.normal(0,1,1) #initial guess\n",
    "    converged = False\n",
    "    while not converged:\n",
    "        b_hat_new = b_hat-eta*MSE_gradient(x,y,b_hat)\n",
    "        if np.abs(b_hat_new-b_hat)<precision_value:\n",
    "            converged = True\n",
    "        b_hat = b_hat_new\n",
    "    return(b_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the gradient descent alogrithm twice. For the first estimate, use a precision value of $0.0001$ and $\\eta$ of $0.00001$ to obtain an estimate of $\\hat{b}$. For the second estimate, use a larger precision value (you choose). Plot and label the first estimated model on the left, the second estimated model in the middle, and the final values of $\\hat{b}$ for each estimate on the error landscape on the right. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonconvex Optimization\n",
    "\n",
    "Let's now try a nonconvex optimization problem. Suppose we are trying to estimate a model of the form:\n",
    "\n",
    "$y = sin(x^b)$\n",
    "\n",
    "where,\n",
    "+ $y$: dependent variable\n",
    "+ $x$: independent variable\n",
    "+ $b$: model parameter to be estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0.001,10,.1)\n",
    "b = 1.3\n",
    "y = np.sin(x**b)\n",
    "plt.plot(x,y)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Sinusoidal Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we have no measurement error, we'll see that this is a challenging optimization problem when using gradient descent.\n",
    "\n",
    "We will once again minimize the MSE in order to estimate the unknown model parameter $b$. The derivative of the MSE is a little more complicated now:\n",
    "\n",
    "$\\frac{\\partial MSE}{\\partial \\hat{b}} = -\\frac{2}{n} \\sum_{i=1}^{n}(y_i-\\hat{y}_i)cos(x_i^{\\hat{b}})ln(x_i)x_i^{\\hat{b}}$\n",
    "\n",
    "Below are two functions that calculate the MSE and the derivative of the MSE with respect to $\\hat{b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(x,y,b_hat):\n",
    "    \"\"\"Calculates the MSE for the sinusoidal model\"\"\"\n",
    "    n = len(x)\n",
    "    y_hat = np.sin(x**(b_hat))\n",
    "    return((1/n)*np.sum((y-y_hat)**2))\n",
    "\n",
    "def MSE_gradient(x,y,b_hat):\n",
    "    \"\"\"Calculates the gradient of the MSE for the sinusoidal model\"\"\"\n",
    "    n = len(x)\n",
    "    y_hat = np.sin(x**b_hat)\n",
    "    return((-2/n)*np.sum((y-y_hat)*np.cos(x**b_hat)*np.log(x)*(x**b_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "**A.** Plot the error landscape and put a marker at the true value of $b$. What issues do you foresee when implementing gradient descent on this function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.** Perform gradient descent 3 times to find $\\hat{b}$. Each time use a different $\\eta$ and precision value of $0.000001$. Plot the error landscape and three estimates of $\\hat{b}$ on the right, and the three estimated models on the left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C.** Why do you think gradient descent performed so poorly on this problem? Did you notice any effects of how you chose your random initial $\\hat{b}$ on alogrithmic performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, classical gradient descent is not used since it is easy to get stuck in local minima. Additionally, it becomes computationally expensive when data sets are large. It is more common to see variations of gradient descent such as *stochastic gradient descent* and the *Adam* optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression via OLS\n",
    "\n",
    "In Exercise 1, we solved a basic linear regression problem with gradient descent. However, the most common method of estimating regression coefficients is to use the ordinary least squares (OLS) solution. The *scikit-learn* library has many useful functions for machine learning, including fitting linear models. Let's simulate some data and find the model coefficients via OLS estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(0,20,100)\n",
    "b0 = 3 #intercept\n",
    "b1 = 2 #slope\n",
    "e = np.random.normal(0,10,len(x))\n",
    "y = b0+b1*x+e\n",
    "\n",
    "plt.figure(figsize = (10,5))\n",
    "plt.plot(x,y,'o')\n",
    "plt.plot(x,b0+b1*x,color = 'black',label = 'True Regression Line')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend()\n",
    "plt.title(\"Regression Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x.reshape((-1,1)) #when only one independent variable, sklearn requires reshaping to a 2-D array\n",
    "model = LinearRegression().fit(X, y) #call linear regression from sklearn\n",
    "b0_hat = model.intercept_ #estimated intercept\n",
    "b1_hat = model.coef_[0] #estimated slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Estimate of b0:\",b0_hat,\"\\nActual b0:\",b0)\n",
    "print(\"\\nEstimate of b1:\",b1_hat,\"\\nActual b1:\",b1)\n",
    "\n",
    "y_hat = b0_hat+b1_hat*x\n",
    "\n",
    "plt.figure(figsize = (10,5))\n",
    "plt.plot(x,y,'o')\n",
    "plt.plot(x,b0+b1*x,color = 'black',label = \"True Regression Line\")\n",
    "plt.plot(x,y_hat,color = 'red', label = \"Estimated Regression Line\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Regression Data\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain predictions of our model using the **model.predict()** function. This function will produce predicted $\\hat{y}$'s for model inputs using the estimated regression coefficients. Using these predictions, we can calculate fit statistics of our model. \n",
    "\n",
    "$R^2$ is the proportion of variance in our dependent variable that can be predicted by our independent variables. We want our $R^2$ to be as close to 1 as possible. Different fields have different criteria for what a \"good\" $R^2$ is. There are several reasons the $R^2$ of a linear regression could be low. These include:\n",
    "\n",
    "+ A linear model was not appropriate.\n",
    "+ The independent variables were not good predictors of our independent variable.\n",
    "+ The is large amount of measurement error.\n",
    "+ OLS assumptions were violated.\n",
    "\n",
    "I encourage you to examine how changing the variance of the measurement error affects the $R^2$ and MSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X)\n",
    "mse = mean_squared_error(y,y_hat)\n",
    "r2 = r2_score(y,y_hat)\n",
    "print(\"Model MSE:\",mse,\"\\nModel R^2:\",r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Scikit-learn* comes with datasets commonly used for practicing machine learning and statistical methods. We will run a multiple regression (i.e. regression with multiple independent variables) using the diabetes dataset, where we will predict disease progression.\n",
    "\n",
    "Below, we will load the data and use the following features:\n",
    "\n",
    "+ $y$: disease progression in patient (low good, high bad)\n",
    "+ $age$: age of patient\n",
    "+ $bmi$: body mass index of patient\n",
    "+ $bp$: patient blood pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = sk.datasets.load_diabetes(return_X_y=True)\n",
    "age = x[:,0]\n",
    "bmi = x[:,2]\n",
    "bp = x[:,3]\n",
    "X = np.array([age,bmi,bp]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "**A.** Plot three scatter plots of the data, where the x-axis of the scatter plot is an independent variable and the y-axis is the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.** Perform a linear regression using the three indepedent variables age, bmi, and bp. Write the regression equation you estimated. Is an increase in age associated with an increase or decrease in disease progression?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C**. Calculate the MSE and $R^2$ of the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D**. In class, we discussed the concept of *regularization*. Ridge regression is a form of regularized regression that corrects for correlations in the data. Rerun the previous analysis using the sklearn Ridge regression model (you should play around with the regularization parameter *alpha*). Calculate the MSE and $R^2$ of this regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at performing classification via logistic regression. We will use the breast cancer dataset to predict the probability of a tumor being malignant ($y = 0$) or benign ($y = 1$) based on tumor features extracted from medical imaging.\n",
    "\n",
    "Recall that for the case where there is a single independent variable, the model for logistic regression is:\n",
    "\n",
    "$p(y=1) = (1+exp[-(b_0 + b_1x)])^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = sk.datasets.load_breast_cancer(return_X_y=True)\n",
    "mean_radius = X[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit a logistic regression model using the mean radius of the tumor as the independent variable. Additionally, we can calculate the **confusion matrix** which tabulates the accuracy of our model.\n",
    "\n",
    "| $ $| $y$ = 0 | $y$ = 1     |\n",
    "| :---        |    :----:   |          ---: |\n",
    "| **$\\hat{y}$ = 0**     | True Negative      | False Negative   |\n",
    "| **$\\hat{y}$ = 1**   | False Positive       | True Positive     |\n",
    "\n",
    "In a perfect world, the confusion matrix would be diagonal. However, you typically have a trade off in the amount of false positives and false negatives. It is your job as a researcher to decide which is worse for a given problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "**A.** Perform a logistic regression predicting $p(y=1)$ using mean radius as the independent variable. Is increased tumor mean radius associated with an increase or decrease in the probability of the tumor being malignant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.** Make a scatter plot with the x-axis mean radius and y-axis class of the tumor. Additionally, plot the sigmoidal curve you obtain from the coefficients you estimated in A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C.** Calculate the confusion matrix (**hint**:confusion_matrix($y,\\hat{y}$)) and use it to find the accuracy of the model. Is this an acceptable amount of false negatives and false positives to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Training Sets\n",
    "\n",
    "An important concept in machine learning (and statistics) is the notion of **overfitting**. This is when your model performs very well on your training data, but has bad performance for out-of-sample data. One solution to detect this is to split your data into **training** and **testing** sets. \n",
    "\n",
    "For this next exercise, we will split our data into training and testing sets and predict tumor class using all of the independent variables in the breast cancer dataset. We will remove a random third of the data to be reserved as the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "**A.** Perform a logistic regression on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.** Calculate the confusion matrix for the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C.** Calculate the confusion matrix for the testing data. Do you think your model was overfitting? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a dimensionality reduction technique that finds a low dimensional representation of our data while preserving the covariance structure. It does this by projecting the data into a new coordinate system which has a lower dimension than the original data space. \n",
    "\n",
    "Each axis (component) in this new coordinate system is found in descending order of variance explained. In other words, the first component explains the most variance in the data and the last component has the smallest amount of variance explained. By dropping components that have very little variance explained, we get a lower dimensional coordinate system.\n",
    "\n",
    "We will use a dataset of handwritten digits to examine how we can use low dimensional structure in a classification task. First, let's load the data from scikit-learn and split into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "data = digits.data\n",
    "labels = digits.target\n",
    "digits_train, digits_test, y_train, y_test = train_test_split(data, labels, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each handwritten digit is given as a vector with 64 elements. We can visualize the data by reshaping each digit into and 8x8 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 4,figsize = (15,3))\n",
    "for i in range(4):\n",
    "    ax[i].imshow(digits_train[i,:].reshape((8,8)),cmap = 'gray',origin='upper')\n",
    "    ax[i].set_title(f'Digit: {y_train[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running PCA, we need to standardize (z-score) our data. If we did not do this, features with more variance would dominate and we may not find the underlying structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score = StandardScaler()\n",
    "X_train = z_score.fit_transform(digits_train)\n",
    "X_test = z_score.fit_transform(digits_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensionality of these digits is 64, but we can use PCA to get a lower dimensional representation (sometimes called a latent space). There are many ways to determine how many components you should keep when performing PCA, but one of the easiest is to look at a *scree plot*.\n",
    "\n",
    "A scree plot shows how much variance is explained by each component in the model. As general rule of thumb, you can use inflection points in the plot to determine how many components you should keep. \n",
    "\n",
    "**Note**: Some people use the term scree plot when plotting the eigenvalues of the components instead of the variance explained. When doing so, people may use the 'eigenvalue greater than one' rule and keep all components with an eigenvalue greater than one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 20 # initially using a large number of components to get a sense of the data\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(X_train)\n",
    "comp_num = range(1,n_components+1)\n",
    "\n",
    "plt.figure(figsize = (10,5))\n",
    "plt.plot(comp_num,pca.explained_variance_ratio_,'--o')\n",
    "plt.xticks(comp_num)\n",
    "plt.ylabel('Ratio of Variance Explained')\n",
    "plt.xlabel('Component')\n",
    "plt.ylim([0,.2])\n",
    "plt.title('Scree Plot of Training Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting the scree plot, let's keep five components for our PCA. We will use the training data to find the components and apply the new coordinate system to both the training and testing data. The **fit_transform** function first fits the PCA model and then transforms the data into the new latent space. The **transform** function only applies the transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 5)\n",
    "\n",
    "train_reduced = pca.fit_transform(X_train)\n",
    "test_reduced = pca.transform(X_test)\n",
    "\n",
    "print(f'Original dimensionality: {X_train.shape[1]}')\n",
    "print(f'New dimensionality: {train_reduced.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common practice to use dimensionality reduction techniques to visualize high dimensional data. In our case, we can use the first two principal components to plot the handwritten digits in a two dimensional representation. A good latent representation will produce similar projections for both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit, axes = plt.subplots(ncols=2, figsize=(13,6), sharex=True, sharey=True)\n",
    "scatter = axes[0].scatter(train_reduced[:,0], train_reduced[:,1], s=5, c=y_train)\n",
    "axes[0].legend(handles=scatter.legend_elements()[0], labels=range(10),loc = 'lower left')\n",
    "axes[0].set_title('Latent Representation of Training Digits')\n",
    "scatter = axes[1].scatter(test_reduced[:,0], test_reduced[:,1], s=5, c=y_test)\n",
    "axes[1].set_title('Latent Representation of Training Digits')\n",
    "axes[0].set_xlabel('PC 1')\n",
    "axes[0].set_ylabel('PC 2')\n",
    "axes[0].set_ylim([-10,10])\n",
    "axes[0].set_xlim([-12,12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)\n",
    "\n",
    "Now that we have a lower dimensional representation of the handwritten digits, we can try and classify the digits using a SVM. \n",
    "\n",
    "The function **svm.SVC** is a SVM classifier. You can specify which kernel you want to use with the **kernel** argument, but we will leave it as linear for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = svm.SVC(kernel = 'linear')\n",
    "svm_model.fit(train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the accuracy of the model using the **predict** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predicted = svm_model.predict(train_reduced)\n",
    "test_predicted = svm_model.predict(test_reduced)\n",
    "\n",
    "training_accuracy = len(train_predicted[train_predicted==y_train])/len(y_train)\n",
    "testing_accuracy = len(test_predicted[test_predicted==y_test])/len(y_test)\n",
    "\n",
    "print(f'Accuracy of classifier on training data: {np.round(training_accuracy*100,2)}%')\n",
    "print(f'Accuracy of classifier on testing data: {np.round(testing_accuracy*100,2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "**A.** Rerun the SVM classifier with a *polynomial* kernel. Give the training and testing accuracy of the new classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.** Rerun the SVM classifier with a *radial basis function* kernel. Give the training and testing accuracy of the new classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C.** Using the *radial basis function* kernel, run the SVM classifier using data that has been projected into a two dimensional space (i.e., n_components = 2). Give the training and testing accuracy of the new classifier. Is there a loss in performance by reducing the dimensionality this substantially?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D.** Rerun the SVM classifier with a *radial basis function* kernel again, but this time use the original 64-dimensional training data. Give the training and testing accuracy of this classifier. Is it much better than using the 5-dimensional representation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
