{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2: Point Process Statistics and Linear Time-Invariant Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week our lab will focus on dealing with point process data in Python and on modeling firing rates as\n",
    "inhomogeneous Poisson processes.\n",
    "\n",
    "Work on the cells marked **Q**. Save the completed notebook as an HTML file and submit on Canvas, one submission per group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load matplotlib inline mode\n",
    "%matplotlib inline\n",
    "\n",
    "# import some useful libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np                # numerical analysis linear algebra\n",
    "import matplotlib.pyplot as plt   # plotting\n",
    "sys.path.insert(0,\"/standard/psyc5270-cdm8j/comp-neurosci\")\n",
    "from comp_neurosci_uva import signal, data, pprox, dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Point processes\n",
    "\n",
    "When we record from neurons, we observe the times when they spike. We can represent these data as an ordered sequence of times in some interval from 0 to $T$:\n",
    "\n",
    "$$\\{0 \\leq t_1 < t_2 < \\ldots < t_N \\leq T\\}$$\n",
    "\n",
    "In contrast to time series, there is not a fixed relationship between the number of events and the duration of the analysis interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When we store point processes in Python or another programming language, we typically use an array, but the elements of the array are event times, not measurements.\n",
    "\n",
    "Because point processes vary in the number of events, multi-channel or multi-trial point-processes are represented by **lists of arrays**, not by 2D arrays.\n",
    "\n",
    "Let's look at an example. The code in the cell below will load spike times recorded from a single neuron in the starling auditory pallium in response to several conspecific songs. Each song is denoted by a short code. We will look at the responses to `A8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = data.load_pprox(Path(\"starling\", \"pprox\", \"st11_1_2_1\"))\n",
    "resp_A8 = pprox.select_stimulus(resp, \"A8\")\n",
    "resp_A8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `resp_A8` refers to a Python **list**. Lists are like arrays, but they can store heterogeneous data types. The syntax for accessing elements and slices is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### I/O for Point Processes\n",
    "\n",
    "There is no standard format for point-process data. Because point process data tend to be smaller than time series, text formats are more common than binary. A very simple text format is to put each trial (or channel) on a separate line and separate the events on each line with a space. Take a look at `data/io-examples/st_11_2_1_A8.txt` for an example.\n",
    "\n",
    "The [PySpike](http://mariomulansky.github.io/PySpike/) library has a function for loading data from such files, but we're going to write our own so that we can learn a bit about basic I/O in Python and looping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list where we will store our trials\n",
    "trials = []\n",
    "# open the file for reading\n",
    "fp = open(data.data_path / \"io-examples\" / \"st11_1_2_1_A8.txt\", mode=\"r\")\n",
    "# loop through the lines of the file with a for statement\n",
    "for line in fp:\n",
    "    # read the line into an array\n",
    "    arr = np.fromstring(line, sep=\" \")\n",
    "    # append the array to our list\n",
    "    trials.append(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you learned how to program in Java or C or another low-level programming language, take a moment to appreciate how simple this task is in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Point process metadata\n",
    "\n",
    "As with time series data, it's important to keep track of metadata. Here are some important metadata that need to be associated with point process files:\n",
    "\n",
    "- type of event (e.g., spike, behavioral action, stimulus start/stop)\n",
    "- number of channels\n",
    "- unit scaling (e.g., milliseconds or seconds?)\n",
    "- start time\n",
    "- other experimental variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spike Train Statistics\n",
    "\n",
    "Recall from last week's lab that we can also represent a spike train as an ordered sequence of spike times:\n",
    "\n",
    "$$X = \\{t, t_1, \\ldots, t_{N-1}\\},$$\n",
    "\n",
    "and that this is a random variable with the joint probability distribution\n",
    "\n",
    "$$p(t_1, \\ldots, t_{N}).$$\n",
    "\n",
    "If we assume that each spike is independent of every other spike, then we have a **Poisson process**,\n",
    "\n",
    "$$p(t_1, \\ldots, t_{N-1}) = \\prod_{i=1}^{N}p(t_i).$$\n",
    "\n",
    "This simplifies things a lot because now we just have to figure out what $p(t_i)$ is.\n",
    "\n",
    "If $p(t_i)$ is constant, the Poisson process is **homogeneous**. In this case, in an interval $(t_i, t_i + \\Delta)$, we would expect to observe $\\lambda = R\\Delta$ events. The distribution of the number of events we actually observe, $n$, is given by the Poisson distribution:\n",
    "\n",
    "$$p(n|\\lambda) = \\frac{\\lambda^n}{n!}\\exp(-\\lambda).$$\n",
    "\n",
    "If $p(t_i)$ depends on time, then the Poisson process is **inhomogeneous** and $\\lambda$ is a function of $t$:\n",
    "\n",
    "$$p(n|\\lambda(t)) = \\frac{\\lambda(t)^n}{n!}\\exp(-\\lambda(t)).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Estimating spike rates\n",
    "\n",
    "Let's think about how we can estimate $\\lambda$ for Poisson processes.\n",
    "\n",
    "If we assume that the process is homogeneous over each trial, then we have a simple observational model where the number of spikes is a random sample from the Poisson distribution:\n",
    "\n",
    "$$p(y_i|\\lambda) = \\frac{\\lambda^n}{n!}\\exp(-\\lambda).$$\n",
    "\n",
    "Given a set of trials, we can estimate $\\lambda$ from the sample mean of the spike count:\n",
    "\n",
    "$$\\hat{\\lambda} = \\frac{T}{N}\\sum_i y_i,$$\n",
    "\n",
    "where $N$ is the number of time bins and $T$ is the duration of the observation interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The problem is a lot trickier if the process is inhomogeneous, because now we're trying to estimate a continuous function of time, $\\lambda(t)$.\n",
    "\n",
    "In this setting, people usually talk about rate ($r$) rather than intensity ($\\lambda$), so we'll use $r(t)$ from here on.\n",
    "\n",
    "The issue we confront is that $r(t)$ is a continuous function. We can discretize it into small intervals of $(t, t + \\Delta)$ and count the number of spike in each interval, but as we make $\\Delta$ smaller to get higher temporal resolution, we reach the point at which each bin has either one or zero spikes, which doesn't tell us much about the rate. We can address this problem by averaging across multiple trials. If we use $\\langle \\rangle$ to denote averaging across trials, this looks like:\n",
    "\n",
    "$$r(t) = \\frac{1}{\\Delta} \\int_t^{t+\\Delta} d\\tau\\; \\langle \\rho(t) \\rangle$$\n",
    "\n",
    "You hopefully can see that as $\\Delta$ gets smaller, the number of trials you need to average to get a smooth function gets larger. So part of our problem is to determine what $\\Delta$ should be. More practically, at what time scale do we think the rate is changing?\n",
    "\n",
    "There are a number of different ways of approximating $r(t)$. We'll look at a couple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spike time histogram\n",
    "\n",
    "For historical reasons, this is also called a peri-stimulus spike time histogram (PSTH), even when there isn't a stimulus.\n",
    "\n",
    "The simplest way of approximating the rate is to divide the interval up into a fixed number of bins of duration $\\Delta$ and count how many spikes occurred in each bin. The rate is simply the number of spikes divided by $\\Delta$.\n",
    "\n",
    "To illustrate, let's generate some trials from an inhomogeneous process that ramps up and then down in rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "trials= 10\n",
    "T     = 100     # s\n",
    "Delta = 0.001   # s\n",
    "N     = int(T / Delta)\n",
    "bins  = np.arange(0, T, Delta)\n",
    "# rate is now a function of time\n",
    "inh_rate  = np.concatenate([np.linspace(0.0, 4.0, N//2),\n",
    "                            np.linspace(4.0, 0.0, N//2)])\n",
    "\n",
    "inh_spikes_v = []\n",
    "inh_spikes_t = []\n",
    "# generate 10 trials\n",
    "for trial in range(trials):\n",
    "    # generate N values from a uniform distribution\n",
    "    rand = dists.uniform().rvs(N)\n",
    "    # this is an alternative method of simulating spiking based on the Bernoulli distribution\n",
    "    # compare each value to lambda = rate * Delta; if it's greater, then the bin gets a spike\n",
    "    lam  = inh_rate * Delta\n",
    "    spike_v = (inh_rate * Delta) > rand\n",
    "    spike_i = np.nonzero(spike_v)[0]\n",
    "    spike_t = bins[spike_v]\n",
    "    inh_spikes_v.append(spike_v)\n",
    "    inh_spikes_t.append(spike_t)\n",
    "inh_spikes_v = np.column_stack(inh_spikes_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the code above and make sure you understand what it's doing. In particular, note that there\n",
    "are two equivalent representations of the spiking data. \n",
    "\n",
    "- `inh_spike_t` contains arrays of spike times. This is akin to our $X = {t_1,\\ldots,t_N}$ formalism.\n",
    "- `inh_spike_v` is a two-dimensional array of 0's and 1's, with the 1's corresponding to times when there were spikes. This is like a time series, a discrete version of $\\rho(t) = \\sum_{i=1}^N \\delta(t - t_i)$. The rows correspond to time and the columns to trials.\n",
    "\n",
    "Each representation lends itself best to a different kind of plot. The spike times are best plotted as a **raster** in which ticks\n",
    "indicate the times when the cell spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(9, 3))\n",
    "\n",
    "for trial in range(trials):\n",
    "    spike_t = inh_spikes_t[trial]\n",
    "    axes.plot(spike_t, np.zeros_like(spike_t) + trial, \"k|\")\n",
    "\n",
    "axes.set_ylabel(\"Rate (Hz)\")\n",
    "axes.set_xlabel(\"Time (s)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could we plot the spike vectors? One option is a **heat map**, where the color indicates the presence or absence of a spike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(9, 3))\n",
    "axes.imshow(inh_spikes_v.T, interpolation=\"none\", aspect=\"auto\", vmin=0, vmax=1, origin=\"lower\", extent=(0, T, 0, trials))\n",
    "\n",
    "axes.set_ylabel(\"Trial\")\n",
    "axes.set_xlabel(\"Time (s)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look very helpful! The problem is that the time step is really small, so there's 100,000 points in each trial.\n",
    "When we try to plot this, the resolution of the image is not high enough to represent every spike, so some get left out.\n",
    "\n",
    "We can plot each vector separately, but what it shows is not very useful. You can sort of tell where the spikes are denser, but not in a way\n",
    "that's at all quantitative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(9, 3))\n",
    "ax.plot(bins, inh_spikes_v[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try generating a histogram with a coarser bin size. For now we'll just use the first trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(9, 3))\n",
    "ax.plot(bins, inh_rate)\n",
    "bin_size = 2.0\n",
    "bin_count = int(T / bin_size)\n",
    "\n",
    "spike_t = inh_spikes_t[0]\n",
    "ax.plot(spike_t, np.zeros_like(spike_t), \"k|\")\n",
    "r_est, edges  = np.histogram(spike_t, bins=np.arange(0, T + bin_size, bin_size))\n",
    "p = ax.step(edges[1:], r_est / bin_size)\n",
    "\n",
    "ax.set_ylabel(\"Rate (Hz)\")\n",
    "ax.set_xlabel(\"Time (s)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The main problem with histograms is that setting the bin size is largely subjective. Try adjusting the `bin size` variable and see what gives you the best tradeoff between variability and temporal resolution.\n",
    "\n",
    "There is still active development of new methods for adaptively setting bin sizes in timing histograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Smoothing\n",
    "\n",
    "Another problem with the PSTH is that the count in each bin depends a lot on where the edges of the bins are.\n",
    "\n",
    "One solution to this problem is to use a **sliding window**. The simplest window is simply a square function with a defined width and a total area equal to 1.0.\n",
    "\n",
    "For example, here's a 2 s window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(3, 3))\n",
    "window_size = 2.0\n",
    "w_t = np.arange(-10.0, 10.0, Delta)\n",
    "w = np.zeros_like(w_t)\n",
    "w[(-window_size/2 < w_t) & (w_t <window_size/2)] = 1. / window_size\n",
    "ax.plot(w_t, w)\n",
    "ax.set_xlabel(\"Time (ms)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convolution\n",
    "\n",
    "We can express the sliding window operation as a sum of the window function for the values of the spike times.\n",
    "\n",
    "$$r(t) \\approx \\sum_{i=0}^{N-1} w(t - t_i)$$\n",
    "\n",
    "This is equivalent to doing an integral over the response function:\n",
    "\n",
    "$$r(t) \\approx \\int_{-\\infty}^{\\infty} d\\tau\\; w(\\tau) \\rho(t - \\tau)$$\n",
    "\n",
    "This integral is also called a linear **convolution** or filter, and we'll be seeing a lot of them.\n",
    "\n",
    "Numpy has a function that can calculate this convolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(9, 3))\n",
    "spike_t = inh_spikes_t[0]\n",
    "spike_v = inh_spikes_v[:,0]\n",
    "ax.plot(spike_t, np.zeros_like(spike_t), \"k|\")\n",
    "r_est = np.convolve(spike_v, w, mode='same')\n",
    "ax.plot(bins, inh_rate)\n",
    "ax.plot(bins, r_est)\n",
    "ax.set_ylabel(\"Rate (Hz)\")\n",
    "ax.set_xlabel(\"Time (s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can use any function as a window as long as it goes to zero outside $\\tau = 0$ and its integral is 1.0.\n",
    "\n",
    "A popular choice is to use a Gaussian, which smooths the function by downweighting points further away from $\\tau = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(3, 3))\n",
    "sigma = 2.0\n",
    "w_t = np.arange(-5 * sigma, 5 * sigma, Delta)\n",
    "w = 1 / np.sqrt(2 * np.pi) / sigma * np.exp(-w_t**2 / 2 / sigma**2)\n",
    "ax.plot(w_t, w)\n",
    "ax.set_xlabel(\"Time (s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(9, 3))\n",
    "\n",
    "ax.plot(bins, inh_rate)\n",
    "ax.plot(spike_t, np.zeros_like(spike_t), \"k|\")\n",
    "\n",
    "r_est = np.convolve(spike_v, w, mode='same')\n",
    "p = ax.plot(bins, r_est)\n",
    "\n",
    "ax.set_ylabel(\"Rate (Hz)\")\n",
    "ax.set_xlabel(\"Time (s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try adjusting the `sigma` variable in the cell that creates the Gaussian window, then run both cells. What\n",
    "value gives a plot that you like the look of?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Averaging trials\n",
    "\n",
    "Hopefully, these exercises have illustrated the fundamental tradeoff between variance and temporal resolution. As you increase the bin (or window) size ($\\Delta$), the estimated rate becomes less variable, but the temporal resolution decreases. Thus, smoothing can interfere with detecting rapid changes in the underlying rate function.\n",
    "\n",
    "As noted above, one solution to this problem is to average across trials. In essence, this gives you multiple independent estimates of the rate at any given instant, thereby reducing the amount of smoothing you need.\n",
    "\n",
    "To apply smoothing to multiple trials, we can either average across trials first and then smooth, or smooth each trial and then average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(9, 5))\n",
    "\n",
    "# smooth first\n",
    "for trial in range(trials):\n",
    "    r_est = np.convolve(inh_spikes_v[:,trial], w, mode='same')\n",
    "    axes[0].plot(bins, r_est)\n",
    "axes[0].set_ylabel(\"Rate (Hz)\")\n",
    "\n",
    "# average first\n",
    "spikes_v = inh_spikes_v.mean(1)\n",
    "r_est = np.convolve(spikes_v, w, mode='same')\n",
    "axes[1].plot(bins, inh_rate)\n",
    "axes[1].plot(bins, r_est)\n",
    "axes[1].set_ylabel(\"Rate (Hz)\")\n",
    "axes[1].set_xlabel(\"Time (s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we bin the spikes and average across trials, we get an estimate of the rate in each bin:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "Consider an inhomogeneous Poisson process with a time-varying rate specified by:\n",
    "\n",
    "$$r(t) = r_\\mathrm{max}[\\sin(2\\pi\\omega t) + 1]$$\n",
    "\n",
    "Use $\\omega$ = 2.1 Hz, $r_\\mathrm{max} = 50$ Hz, and a response interval from 0 to 2 s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1** Plot r(t) with a time step of 1 ms (0.001 s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2** Generate 20 independently simulated spike trains and plot them as rasters. There is code in previous notebooks you can use to make the raster plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3** Using a bin size of 10 ms, calculate the PSTHs averaged from the first 10 trials and the last 10 trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4** Now simulate 1000 trials and calculate a PSTH from the first and second half. How do these PSTHs relate to $\\lambda(t)$? Do more trials give you a more precise estimate of the rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5** Calculate a PSTH *for each trial* of the 1000-trial simulation. This will yield a 1000 x 200 array, with trials along one axis and time along the other. For each time bin, calculate and plot the mean, variance, and Fano factor. Each of these will be a 200-element array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.6** Repeat the analysis in the previous question but with a bin size of 150 ms. What happens to the Fano factor? Why is it important to choose a bin size such that the rate is not changing much within each bin?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear time-invariant systems\n",
    "\n",
    "Consider a dynamical system with input $x(t)$ and output $y(t)$.\n",
    "\n",
    "![dynamical system](https://meliza.org/public/courseware/comp-neurosci/images/l6_dynamical_system.png)\n",
    "\n",
    "The system is **linear** if it obeys the laws of superposition and scaling. That is, if\n",
    "\n",
    "\\begin{align}\n",
    "a(t) & \\rightarrow b(t) \\\\\n",
    "c(t) & \\rightarrow d(t)\n",
    "\\end{align}\n",
    "\n",
    "Then the following must be true:\n",
    "\n",
    "\\begin{align}\n",
    "\\alpha a(t) & \\rightarrow \\alpha b(t) \\\\\n",
    "\\alpha a(t) + \\beta c(t) & \\rightarrow \\alpha b(t) + \\beta d(t)\n",
    "\\end{align}\n",
    "\n",
    "Furthemore, for the system to be **time-invariant**, shifting the input in time will lead to the same output, shifted in time by an equal amount:\n",
    "\n",
    "\\begin{align}\n",
    "a(t + \\Delta) & \\rightarrow b(t + \\Delta)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Impulse Response Functions\n",
    "\n",
    "How can we characterize the transformation that occurs between the input and output of an LTI system?\n",
    "\n",
    "It turns out, we only need to know one thing: the system's **impulse response function**. This is the response you would obtain to a small pulse of unit amplitude, $\\delta(t) \\rightarrow h(t)$.\n",
    "\n",
    "For example, here's the impulse response function of a tuning fork. When you hit it, it starts oscillating at 100 Hz and then decays (rather quickly, for illustration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0.0, 1.0, 1000)\n",
    "h = np.sin(100 * t) * np.exp(- 5 * t)\n",
    "plt.plot(t, h)\n",
    "plt.xlabel(\"Time (s)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once we know $h(t)$, we can compute the output $y(t)$ to **any** input $x(t)$. Why? Because any signal can be represented as a sum of time-shifted unit impulses of varying amplitude:\n",
    "\n",
    "$$x(t) \\equiv \\sum_i \\delta(t - \\tau_i) x(\\tau_i)$$\n",
    "\n",
    "Recall that $\\delta(t)$ is equal to $\\infty$ at $t=0$, is zero everywhere else, and that the area under $\\delta(t)$ is equal to 1. $\\delta(t - \\tau_i)$ simply shifts the impulse to $\\tau_i$, and $x(\\tau_i)$ scales it by the value of $x(t)$ at $t = \\tau_i$. \n",
    "\n",
    "Although this equation seems trivial, it's the basis of **discretization** and an important operation called **convolution**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: the python keyword `lambda` allows us to define simple functions inline\n",
    "f = lambda t: np.sin(2 * np.pi * t)\n",
    "plt.plot(t, f(t))\n",
    "tau = np.linspace(0.0, 1.0, 10)\n",
    "plt.vlines(tau, 0, f(tau))\n",
    "plt.plot(tau, f(tau), 'ko')\n",
    "plt.xlabel(\"Time (s)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution\n",
    "\n",
    "So how do we predict the response of our system from $h(t)$?\n",
    "\n",
    "Because the system is time-invariant, we know the response to an impulse shifted by $\\tau_i$ is just $h(t)$ shifted by the same amount:\n",
    "\n",
    "\\begin{align}\n",
    "\\delta(t - \\tau_i) & \\rightarrow h(t - \\tau_i)\n",
    "\\end{align}\n",
    "\n",
    "If this impulse is scaled by $x(\\tau_i)$, then the output is scaled equally:\n",
    "\n",
    "\\begin{align}\n",
    "x(\\tau_i) \\delta(t - \\tau_i) & \\rightarrow x(\\tau_i) h(t - \\tau_i)\n",
    "\\end{align}\n",
    "\n",
    "Finally, because of superposition, if we add together many scaled, time-shifted impulses, the output is simply the sum of the scaled, time-shifted responses:\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_i x(\\tau_i) \\delta(t - \\tau_i) & \\rightarrow \\sum_i x(\\tau_i) h(t - \\tau_i)\n",
    "\\end{align}\n",
    "\n",
    "So,\n",
    "\n",
    "$$y(t) \\equiv \\sum_i h(t - \\tau_i) x(\\tau_i)$$\n",
    "\n",
    "Here's an illustration for the sine function in the previous cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=1, sharex=True, figsize=(9, 6))\n",
    "tot = np.zeros(t.size + h.size)\n",
    "axes[0].vlines(tau, 0, f(tau))\n",
    "axes[0].plot(tau, f(tau), 'ko')\n",
    "axes[0].set_title(r\"$x(\\tau_i)$\")\n",
    "# this loop is implementing the summation in the convolution equation\n",
    "for tau_i in tau:\n",
    "    hf = h * f(tau_i)\n",
    "    axes[1].plot(t + tau_i, hf)\n",
    "    idx = t.searchsorted(tau_i)\n",
    "    tot[idx:idx+hf.size] += hf\n",
    "axes[2].plot(np.linspace(0, 2.0, tot.size), tot)\n",
    "axes[1].set_title(r\"$h(t - \\tau_i)x(\\tau_i)$\")\n",
    "axes[2].set_title(r\"$\\sum h(t - \\tau_i)x(\\tau_i)$\")\n",
    "axes[2].set_xlabel(\"Time (s)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how constructive and destructive interference produces a rather complex pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convolution (II)\n",
    "\n",
    "As the spacing between samples becomes infinitesimally small, the convolution sum becomes an integral:\n",
    "\n",
    "$$y(t) = \\int_{-\\infty}^\\infty h(t - \\tau) x(\\tau) d\\tau$$\n",
    "\n",
    "The shorthand for convolution is $*$ : $y(t) = (h * t)(t)$\n",
    "\n",
    "Convolution is commutative:\n",
    "\n",
    "\\begin{align}\n",
    "(h * t)(t) & = (t * h)(t) \\\\\n",
    "\\sum_i h(t - \\tau_i) x(\\tau_i) & = \\sum_i h(\\tau_i) x(t - \\tau_i)\n",
    "\\end{align}\n",
    "\n",
    "Convolution can be done in any domain.\n",
    "\n",
    "One way of interpreting the convolution sum is that it tells us that the output is computed by taking a *weighted sum of the present and past input values*. We can see this by writing out the sum:\n",
    "\n",
    "$$\\sum_i h(\\tau_i) x(t - \\tau_i) = h(0)x(t) + h(1)x(t - 1) + \\cdots$$\n",
    "\n",
    "The system is **causal** if $h(\\tau)$ is only greater than zero for $\\tau \\geq 0$.\n",
    "\n",
    "In most physical systems, the impulse response decays away with time, so there is a point where we can consider $h(\\tau)$ to be essentially zero and truncate the function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here are some visual illustrations of convolution from [Wikipedia](https://en.wikipedia.org/wiki/Convolution). Essentially you are taking one of the functions, flipping it in time, sliding it past the other function, and computing the area where the two functions overlap. Convolution is also called **filtering**.\n",
    "\n",
    "![convolution_animation](https://meliza.org/public/courseware/comp-neurosci/images/l6_convolution_box.gif)\n",
    "\n",
    "![convolution_diagram](https://meliza.org/public/courseware/comp-neurosci/images/l6_convolution_static.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LTI neuron models\n",
    "\n",
    "We now have our first model of how sensory neurons respond to stimuli.\n",
    "\n",
    "In essense, we are representing the neuron as a linear filter that computes a weighted sum of the stimulus as it varies in time.\n",
    "\n",
    "Although very simple, LTI models (and linear filters) can generate surprisingly complex behavior. The exercises in this notebook will help you explore some of this complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "We will investigate the properties of two different LTI models. Their impulse response functions are:\n",
    "\n",
    "\\begin{align}\n",
    "h_1(t) & = \n",
    "\\frac{t}{\\tau_1^2} e^{(-t/\\tau_1)} \\\\\n",
    "h_2(t) & = \n",
    "\\frac{t}{\\tau_1^2} e^{(-t/\\tau_1)} - \\frac{t}{\\tau_2^2} e^{(-t/\\tau_2)}\n",
    "\\end{align}\n",
    "\n",
    "Both filters are causal, so $h_1(t) = h_2(t) = 0$ for all $t < 0$.\n",
    "\n",
    "Functions with the general form of $t \\exp (-t)$ are called **alpha** functions. To get you started, I've defined a Python *function* that will generate alpha kernels for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha(tau, duration, dt):\n",
    "    \"\"\"An alpha function kernel with time constant tau, scaled to \n",
    "    \n",
    "    tau: the time constant of the kernel (in units of duration/dt)\n",
    "    duration: the duration of the support for the kernel\n",
    "    dt: the sampling interval of the kernel\n",
    "    \n",
    "    Returns a tuple (h(t), h(t))\n",
    "    \"\"\"\n",
    "    t = np.arange(0, duration, dt)\n",
    "    k = t / tau**2 * np.exp(-t / tau)\n",
    "    return (k, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Like mathematical functions, Python functions take one or more **arguments**. The result of applying the function to those arguments is the **return value**. Functions can only return a single value, but we can easily get around this by returning a **tuple**. In Python, a tuple is a kind of list that can't be modified. You can unpack the tuple into separate variables when you call a function by using **deconstruction**, as illustrated in the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1, t = alpha(50, 1000, 1)\n",
    "plt.plot(t, h1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** Let $\\tau_1 = 50$ ms and $\\tau_2 = 100$ ms. Plot $h_1(t)$ and $h_2(t)$ for $0 < t < 1000$ ms, with a time step of 1 ms. Use the cell above as a model for your code. Note that $h_2(t)$ is the *sum* of $h_1(t)$ and another alpha function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**2.2** Consider three input signals:\n",
    "\n",
    "\\begin{align}\n",
    "s_1(t) & = \\sin(2 \\pi \\omega_1 t) \\\\\n",
    "s_2(t) & = \\sin(2 \\pi \\omega_2 t) \\\\\n",
    "s_3(t) & = \\mathrm{sign}\\; [s_1(t)]\n",
    "\\end{align}\n",
    "\n",
    "Let $\\omega_1 = 0.3$ Hz and $\\omega_2 = 3$ Hz. For $s_3$, `sign` means that the value is 1.0 if $s_1(t) > 0$ and -1.0 if $s_1(t) \\leq 0$.\n",
    "\n",
    "Generate and plot 10 s of data for each signal, using a time step of 1 ms. Keep your time units consistent!\n",
    "\n",
    "Hint: Use `plt.subplots` to generate a nice grid of plots (see above for examples)\n",
    "\n",
    "Another hint: Don't reinvent the wheel! See if there's a numpy function that will compute `sign` for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**2.3A.** Convolve $s_1$ with the $h_1$ impulse response functions and plot the results. Do this by writing your own loop. See the model above for an example of how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**2.3B.** Now use `np.convolve` to compute the convolution of $s_1$ and $h_1$. Do you get the same result as when you did it manually? If not, you might need to play with the `mode` argument to `np.convolve`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**2.3C.** Now do the convolution for each combination of signal ($s_1$, $s_2$, and $s_3$) and kernel ($h_1$ and $h_2$) and plot the results.\n",
    "\n",
    "Hint: Use `plt.subplots` to generate a grid of 6 panels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3D.** What do you notice about the response amplitudes? What differences do you see between the outputs of the two filters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**2.4.** Now let's consider a white noise input:\n",
    "\n",
    "$$s_4(t) \\sim N(0,1)$$\n",
    "\n",
    "$N$ means that each sample is drawn from a normal distribution with mean 0 and standard deviation 1.0. (Hint: use `np.random.randn`)\n",
    "\n",
    "Compute and plot $(h_1 * s_4)$ and $(h_2 * s_4)$, with $s_4(t)$ evaluated over a 10 s interval with resolution of 1 ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**2.5.** It's a bit hard to compare the results of the convolution in the time domain, so let's see what the spectrum looks like.\n",
    "\n",
    "We'll discuss spectral analysis in some detail later in the course, but for now I've provided the code you need. Just change the variable names to match what you used in the previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "# s4 should be your white noise signal\n",
    "freq, S4 = signal.welch(s4, nperseg=10000, fs=1000)\n",
    "plt.plot(freq, S4, 'k:', label=r\"$s_4$\")\n",
    "# the variable h1s4 should contain the convolution of s_4 with h_1\n",
    "freq, H1S4 = signal.welch(h1s4, nperseg=10000, fs=1000)\n",
    "plt.plot(freq, H1S4, label=r\"$h_1 * s_4$\")\n",
    "# h2s4 = h_2 * s_4\n",
    "freq, H2S4 = signal.welch(h2s4, nperseg=10000, fs=1000)\n",
    "plt.plot(freq, H2S4, label=r\"$h_2 * s_4$\")\n",
    "plt.xlim(0, 20)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Frequency (Hz)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5A** What do you notice about the spectra? Do they seem noisy? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5B** Use a 1000-s input signal to to get a better estimate of the spectrum.\n",
    "\n",
    "You can copy the code in the cell above to calculate and plot the spectra, but you'll need to write the code for generating the longer signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5C.** Now that we have a nice plot, describe the following:\n",
    "\n",
    "- the shape of the spectrum for the input signal\n",
    "- the shape of the spectra for the two convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "Is it possible to estimate the impulse response function from the output of an LTI system when then input is *not* an impulse?\n",
    "\n",
    "Yes! The opposite operation of convolution is called **correlation** or **cross-correlation**\n",
    "\n",
    "$$(a \\star b)(t) = \\sum_i a(t + \\tau_i) b(t)$$\n",
    "\n",
    "Notice how similar the definition is to that of convolution. However, there is a critical sign difference: in convolution the \"sliding\" function is inverted in time, in correlation it is not.\n",
    "\n",
    "<img src=\"https://gracula.psyc.virginia.edu/public/courseware/comp-neurosci/images/l6_correlation_static.png\" alt=\"correlation_diagram\" style=\"width: 300px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6** Compute and plot the correlation between $s_4(t)$ and $(s_4 * h_1)(t)$, and between $s_4(t)$ and $(s_4 * h_2)(t)$.\n",
    "\n",
    "Use `np.correlate` with the argument `mode=\"same\"`. Note that although *we* know $h_1$ and $h_2$ are causal, the correlation function does not. The output of `np.correlate` therefore contains both the causal ($t \\geq 0$) and acausal ($t < 0$) components.\n",
    "\n",
    "Try using 10 seconds of data first and then 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the outputs of the correlation compare to the original $h_1$ and $h_2$ kernels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if you reverse the order of the arguments to `correlate`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
